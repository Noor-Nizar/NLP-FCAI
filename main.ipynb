{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Natural_language_processing\" \n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# get text from headings:\n",
    "text_elements = soup.find_all(['p', 'h1', 'h2', 'h3'])\n",
    "text = \"\"\n",
    "for element in text_elements:\n",
    "    text += element.get_text() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8637\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noornizar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/noornizar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/noornizar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cleaning\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "\n",
    "# Normalization\n",
    "normalized_text = clean_text.lower() \n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(normalized_text)\n",
    "\n",
    "# Stemming \n",
    "stemmer =  nltk.stem.PorterStemmer()\n",
    "tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Lemmatization \n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'three', 'divis', 'languag', 'perspect', 'psycholinguist', 'follow', 'pursu', 'subdivid', 'colleg', 'possibl', 'acl', 'explicit', 'major', 'devi', 'veri', 'semant', 'note', 'conveni', 'rulebas', 'frequent', 'studi', 'extract', 'analyz', 'base', 'categor', 'inform', 'address', 'patient', 'otherwis', 'linksedit', 'karl', 'especi', 'contain', 'speak', 'content', 'idea', 'nuanc', 'introduct', 'given', 'among', 'approach', 'coauthor', 'well', 'million', 'old', 'steadi', 'sequencetosequ', 'becam', 'speech', 'probabilist', 'electron', 'import', 'though', 'realworld', 'wellsummar', 'winter', 'multimod', 'close', 'flurri', 'chine', 'turn', 'variou', 'thought', 'broadli', 'learn', 'abil', 'mikolov', 'discours', 'categori', 'time', 'tool', 'markov', 'howev', 'tag', 'test', 'best', 'appli', 'match', 'find', 'singl', 'partofspeech', 'machin', 'result', 'earliest', 'announc', 'recognit', 'document', 'articl', 'student', 'notion', 'help', 'tasksedit', 'historyedit', 'individu', 'mainstream', 'tree', 'explain', 'extrapol', 'wa', 'increasingli', 'seri', 'inher', 'corpus', 'record', 'statist', 'support', 'protect', 'yoshua', 'end', 'thi', 'ai', 'room', 'syntact', 'experi', 'extern', 'replac', 'relat', 'function', 'seek', 'j', 'limit', 'ifthen', 'networkbas', 'achiev', 'answer', 'par', 'healthcar', 'ngram', 'alan', 'discourag', 'contextu', 'handwritten', 'autom', 'stem', 'either', 'organ', 'intermedi', 'hand', 'care', 'grammar', 'mid', 'sentencesedit', 'build', 'confront', 'recurr', 'british', 'went', 'readingedit', 'write', 'exampl', 'energi', 'handcod', 'need', 'root', 'abov', 'hard', 'involv', 'brno', 'combin', 'late', 'interdisciplinari', 'share', 'uptak', 'approachedit', 'subtask', 'strong', 'commonli', 'knowledg', 'understand', 'start', 'operationaliz', 'georg', 'direct', 'whose', 'advanc', 'measur', 'articul', 'lakoff', 'properti', 'use', 'represent', 'medicin', 'power', 'friston', 'simpl', 'subfield', 'includ', 'insight', 'word', 'linguist', 'technic', 'becom', 'eg', 'interpret', 'reviv', 'first', 'machineri', 'part', 'toward', 'confer', 'framework', 'train', 'john', 'development', 'serv', 'neural', 'morpholog', 'law', 'scienc', 'capabl', 'criterion', 'thennewlyinv', 'rare', 'onli', 'ssedit', 'topic', 'higherlevel', 'conll', 'presenc', 'depend', 'directionsedit', 'methodolog', 'dictionari', 'dure', 'moor', 'longstand', 'underli', 'privaci', 'cognitionedit', 'coars', 'construct', 'intertwin', 'action', 'befor', 'phrasebook', 'symbol', 'embed', 'appar', 'collect', 'made', 'mani', 'nevertheless', 'networkstyl', 'elabor', 'problem', 'advantag', 'contextedit', 'behaviour', 'neuroscientist', 'histor', 'area', 'searl', 'london', 'comprehens', 'domin', 'along', 'concern', 'solv', 'actr', 'give', 'sedit', 'ie', 'gener', 'techniqu', 'technolog', 'machinelearn', 'tendenc', 'within', 'earli', 'larger', 'inaccess', 'improv', 'produc', 'scientif', 'ha', 'alsoedit', 'see', 'mental', 'premis', 'theoret', 'acquir', 'coupl', 'applic', 'field', 'drawback', 'sinc', 'sort', 'text', 'cognit', 'complex', 'data', 'natur', 'captur', 'would', 'question', 'lessen', 'sen', 'futur', 'tie', 'particular', 'cluster', 'period', 'defin', 'task', 'method', 'transform', 'mind', 'maintain', 'context', 'le', 'primarili', 'list', 'widespread', 'perceptron', 'caus', 'popular', 'bengio', 'health', 'phd', 'ineffici', 'manipul', 'networksedit', 'analysisedit', 'year', 'challeng', 'anymor', 'stateoftheart', 'previous', 'underpin', 'refer', 'align', 'hidden', 'set', 'chomskyan', 'due', 'operation', 'revolut', 'human', 'neurosci', 'new', 'likewis', 'age', 'llm', 'common', 'trend', 'accur', 'show', 'process', 'although', 'themselv', 'obsolet', 'increas', 'repres', 'system', 'sever', 'lexic', 'publish', 'step', 'specif', 'requir', 'separ', 'theori', 'cpu', 'call', 'wordvec', 'partli', 'necessari', 'propos', 'offer', 'free', 'mostli', 'trajectori', 'aspect', 'translat', 'gradual', 'similar', 'beyond', 'processingedit', 'presentedit', 'heritag', 'psycholog', 'branch', 'multilay', 'dataset', 'length', 'principl', 'one', 'naturallanguag', 'univers', 'lookup', 'referencesedit', 'goal', 'alreadi', 'two', 'larg', 'artifici', 'ture', 'emul', 'intellig', 'engin', 'still', 'layer', 'observ', 'research', 'comput', 'nlp', 'aid', 'titl', 'heurist', 'develop', 'corpu', 'applicationsedit', 'model', 'theoretician', 'recent', 'decis', 'tom', 'network', 'overperform', 'algorithm', 'featur', 'deep', 'rule'}\n",
      "423\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(filtered_tokens)\n",
    "print(unique_words) \n",
    "print(len(unique_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtpqt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
