{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Natural_language_processing\" \n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# get text from headings:\n",
    "text_elements = soup.find_all(['p', 'h1', 'h2', 'h3'])\n",
    "text = \"\"\n",
    "for element in text_elements:\n",
    "    text += element.get_text() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noornizar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/noornizar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/noornizar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cleaning\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "\n",
    "# Normalization\n",
    "normalized_text = clean_text.lower() \n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(normalized_text)\n",
    "\n",
    "# Lemmatization \n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Stop Words Removal\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'increase', 'principle', 'problem', 'inherent', 'extract', 'wa', 'decision', 'handwritten', 'external', 'syntactic', 'lessening', 'modeling', 'obsolete', 'machine', 'turn', 'corpus', 'sedit', 'functional', 'development', 'serve', 'document', 'along', 'aid', 'capture', 'concerned', 'one', 'following', 'approachedit', 'artificial', 'theoretical', 'analyze', 'bengio', 'seeking', 'revived', 'became', 'text', 'field', 'acquiring', 'question', 'machinelearning', 'test', 'earliest', 'several', 'machinery', 'increasingly', 'analysisedit', 'linguistics', 'interdisciplinary', 'early', 'notion', 'cognitionedit', 'towards', 'word', 'accurately', 'patient', 'involve', 'would', 'operationalizable', 'length', 'article', 'large', 'sentencesedit', 'ended', 'major', 'time', 'organize', 'tasksedit', 'discouraged', 'processing', 'possible', 'experience', 'theory', 'strong', 'including', 'due', 'period', 'mids', 'directionsedit', 'medicine', 'context', 'language', 'million', 'alignment', 'require', 'important', 'technology', 'refers', 'le', 'parsing', 'primarily', 'learning', 'tom', 'example', 'friston', 'result', 'needed', 'see', 'achieve', 'readingedit', 'british', 'individual', 'healthcare', 'series', 'challenge', 'task', 'nuance', 'premise', 'various', 'intertwined', 'subtasks', 'although', 'actr', 'separate', 'conference', 'commonly', 'deep', 'higherlevel', 'cognition', 'pursued', 'direction', 'branch', 'frequently', 'future', 'science', 'application', 'mental', 'lookup', 'george', 'process', 'john', 'partly', 'broadly', 'defining', 'technically', 'study', 'multimodal', 'subfield', 'include', 'specifically', 'ssedit', 'thought', 'neuroscientist', 'modelling', 'student', 'morphological', 'rule', 'hand', 'similar', 'semantic', 'offer', 'symbol', 'support', 'historical', 'likewise', 'list', 'presence', 'stateoftheart', 'giving', 'end', 'translation', 'underlies', 'alsoedit', 'inaccessible', 'called', 'ngram', 'algorithm', 'action', 'ifthen', 'ha', 'trend', 'contained', 'computer', 'note', 'late', 'goal', 'common', 'human', 'networkstyle', 'become', 'complex', 'however', 'understanding', 'within', 'free', 'mainstream', 'wellsummarized', 'went', 'trajectory', 'knowledge', 'extrapolate', 'articulated', 'idea', 'categorize', 'chinese', 'method', 'tagging', 'necessary', 'tree', 'though', 'manipulating', 'cpu', 'theoretician', 'natural', 'otherwise', 'searles', 'made', 'tool', 'computational', 'capable', 'answer', 'subdivided', 'confronts', 'automated', 'announced', 'approach', 'showing', 'model', 'university', 'well', 'power', 'anymore', 'operationalization', 'finding', 'college', 'linksedit', 'dominance', 'area', 'conll', 'probabilistic', 'feature', 'phrasebook', 'popularity', 'applying', 'discourse', 'widespread', 'replaced', 'research', 'perspective', 'larger', 'old', 'direct', 'networkbased', 'psychology', 'first', 'psycholinguistics', 'property', 'step', 'uptake', 'still', 'generation', 'perceptron', 'observed', 'rarely', 'particular', 'build', 'relational', 'many', 'interpretation', 'new', 'involves', 'convenience', 'set', 'energy', 'acl', 'transformation', 'featuring', 'shared', 'two', 'recognition', 'comprehension', 'tie', 'ai', 'handcoding', 'advanced', 'category', 'intelligent', 'alan', 'advance', 'previously', 'lexical', 'phd', 'framework', 'processingedit', 'datasets', 'mind', 'among', 'turing', 'referencesedit', 'intelligence', 'researched', 'beyond', 'embeddings', 'develop', 'health', 'partofspeech', 'rulebased', 'improve', 'others', 'especially', 'inefficiency', 'limited', 'tendency', 'realworld', 'trained', 'naturallanguage', 'brno', 'represents', 'computing', 'heuristic', 'contextual', 'overperformed', 'presentedit', 'markov', 'measured', 'recently', 'transformational', 'help', 'underpinnings', 'protect', 'nlp', 'neural', 'mikolov', 'maintained', 'multilayer', 'grammar', 'stemming', 'chomskyan', 'llm', 'contextedit', 'steady', 'system', 'neuroscience', 'manipulate', 'semantics', 'speaking', 'wordvec', 'technique', 'using', 'lakoff', 'elaborate', 'based', 'introduction', 'moore', 'behaviour', 'advantage', 'closely', 'addressed', 'drawback', 'hidden', 'emulates', 'representation', 'privacy', 'record', 'engineering', 'longstanding', 'producing', 'heritage', 'applied', 'j', 'content', 'gradual', 'general', 'single', 'proposed', 'collection', 'apparent', 'explainability', 'flurry', 'coupled', 'ability', 'starting', 'winter', 'combining', 'applicationsedit', 'networksedit', 'recurrent', 'cognitive', 'solving', 'use', 'sort', 'sequencetosequence', 'technical', 'data', 'writing', 'room', 'aspect', 'part', 'electronic', 'methodology', 'includes', 'yoshua', 'division', 'year', 'explicit', 'caused', 'symbolic', 'three', 'experiment', 'karl', 'published', 'eg', 'nevertheless', 'mostly', 'topic', 'dictionary', 'devising', 'since', 'already', 'used', 'layer', 'whose', 'dependency', 'ie', 'historyedit', 'developmental', 'root', 'best', 'coauthor', 'coarse', 'either', 'london', 'insight', 'age', 'intermediate', 'care', 'law', 'thennewlyinvented', 'scientific', 'titled', 'historically', 'sens', 'speech', 'revolution', 'network', 'criterion', 'simple', 'hard', 'matching', 'construction', 'emulate', 'statistical', 'information', 'cluster', 'given'}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(filtered_tokens)\n",
    "print(unique_words) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtpqt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
